{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.5.2",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    }
  },
  "nbformat": 4,
  "cells": [
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "metadata": {},
      "source": [
        "\nDeep Learning for Natural Language Processing with Pytorch\n**********************************************************\n**Author**: `Robert Guthrie <https://github.com/rguthrie3/DeepLearningForNLPInPytorch>`_\n\nThis tutorial will walk you through the key ideas of deep learning\nprogramming using Pytorch. Many of the concepts (such as the computation\ngraph abstraction and autograd) are not unique to Pytorch and are\nrelevant to any deep learning tool kit out there.\n\nI am writing this tutorial to focus specifically on NLP for people who\nhave never written code in any deep learning framework (e.g, TensorFlow,\nTheano, Keras, Dynet). It assumes working knowledge of core NLP\nproblems: part-of-speech tagging, language modeling, etc. It also\nassumes familiarity with neural networks at the level of an intro AI\nclass (such as one from the Russel and Norvig book). Usually, these\ncourses cover the basic backpropagation algorithm on feed-forward neural\nnetworks, and make the point that they are chains of compositions of\nlinearities and non-linearities. This tutorial aims to get you started\nwriting deep learning code, given you have this prerequisite knowledge.\n\nNote this is about *models*, not data. For all of the models, I just\ncreate a few test examples with small dimensionality so you can see how\nthe weights change as it trains. If you have some real data you want to\ntry, you should be able to rip out any of the models from this notebook\nand use them on it.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "1. Introduction to Torch's tensor library\n=========================================\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "All of deep learning is computations on tensors, which are\ngeneralizations of a matrix that can be indexed in more than 2\ndimensions. We will see exactly what this means in-depth later. First,\nlets look what we can do with tensors.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Creating Tensors\n~~~~~~~~~~~~~~~~\n\nTensors can be created from Python lists with the torch.Tensor()\nfunction.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Create a torch.Tensor object with the given data.  It is a 1D vector\nV_data = [1., 2., 3.]\nV = torch.Tensor(V_data)\nprint(V)\n\n# Creates a matrix\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.Tensor(M_data)\nprint(M)\n\n# Create a 3D tensor of size 2x2x2.\nT_data = [[[1., 2.], [3., 4.]],\n          [[5., 6.], [7., 8.]]]\nT = torch.Tensor(T_data)\nprint(T)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "What is a 3D tensor anyway? Think about it like this. If you have a\nvector, indexing into the vector gives you a scalar. If you have a\nmatrix, indexing into the matrix gives you a vector. If you have a 3D\ntensor, then indexing into the tensor gives you a matrix!\n\nA note on terminology:\nwhen I say \"tensor\" in this tutorial, it refers\nto any torch.Tensor object. Matrices and vectors are special cases of\ntorch.Tensors, where their dimension is 1 and 2 respectively. When I am\ntalking about 3D tensors, I will explicitly use the term \"3D tensor\".\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Index into V and get a scalar\nprint(V[0])\n\n# Index into M and get a vector\nprint(M[0])\n\n# Index into T and get a matrix\nprint(T[0])"
      ]
    },
    {
      "metadata": {},
      "source": [
        "You can also create tensors of other datatypes. The default, as you can\nsee, is Float. To create a tensor of integer types, try\ntorch.LongTensor(). Check the documentation for more data types, but\nFloat and Long will be the most common.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "You can create a tensor with random data and the supplied dimensionality\nwith torch.randn()\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = torch.randn((3, 4, 5))\nprint(x)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Operations with Tensors\n~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can operate on tensors in the ways you would expect.\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = torch.Tensor([1., 2., 3.])\ny = torch.Tensor([4., 5., 6.])\nz = x + y\nprint(z)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "See `the documentation <http://pytorch.org/docs/torch.html>`__ for a\ncomplete list of the massive number of operations available to you. They\nexpand beyond just mathematical operations.\n\nOne helpful operation that we will make use of later is concatenation.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# By default, it concatenates along the first axis (concatenates rows)\nx_1 = torch.randn(2, 5)\ny_1 = torch.randn(3, 5)\nz_1 = torch.cat([x_1, y_1])\nprint(z_1)\n\n# Concatenate columns:\nx_2 = torch.randn(2, 3)\ny_2 = torch.randn(2, 5)\n# second arg specifies which axis to concat along\nz_2 = torch.cat([x_2, y_2], 1)\nprint(z_2)\n\n# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n# torch.cat([x_1, x_2])"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Reshaping Tensors\n~~~~~~~~~~~~~~~~~\n\nUse the .view() method to reshape a tensor. This method receives heavy\nuse, because many neural network components expect their inputs to have\na certain shape. Often you will need to reshape before passing your data\nto the component.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 3, 4)\nprint(x)\nprint(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n# Same as above.  If one of the dimensions is -1, its size can be inferred\nprint(x.view(2, -1))"
      ]
    },
    {
      "metadata": {},
      "source": [
        "2. Computation Graphs and Automatic Differentiation\n===================================================\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "The concept of a computation graph is essential to efficient deep\nlearning programming, because it allows you to not have to write the\nback propagation gradients yourself. A computation graph is simply a\nspecification of how your data is combined to give you the output. Since\nthe graph totally specifies what parameters were involved with which\noperations, it contains enough information to compute derivatives. This\nprobably sounds vague, so lets see what is going on using the\nfundamental class of Pytorch: autograd.Variable.\n\nFirst, think from a programmers perspective. What is stored in the\ntorch.Tensor objects we were creating above? Obviously the data and the\nshape, and maybe a few other things. But when we added two tensors\ntogether, we got an output tensor. All this output tensor knows is its\ndata and shape. It has no idea that it was the sum of two other tensors\n(it could have been read in from a file, it could be the result of some\nother operation, etc.)\n\nThe Variable class keeps track of how it was created. Lets see it in\naction.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Variables wrap tensor objects\nx = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True)\n# You can access the data with the .data attribute\nprint(x.data)\n\n# You can also do all the same operations you did with tensors with Variables.\ny = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True)\nz = x + y\nprint(z.data)\n\n# BUT z knows something extra.\nprint(z.creator)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "So Variables know what created them. z knows that it wasn't read in from\na file, it wasn't the result of a multiplication or exponential or\nwhatever. And if you keep following z.creator, you will find yourself at\nx and y.\n\nBut how does that help us compute a gradient?\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Lets sum up all the entries in z\ns = z.sum()\nprint(s)\nprint(s.creator)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "So now, what is the derivative of this sum with respect to the first\ncomponent of x? In math, we want\n\n\\begin{align}\\frac{\\partial s}{\\partial x_0}\\end{align}\n\n\n\nWell, s knows that it was created as a sum of the tensor z. z knows\nthat it was the sum x + y. So\n\n\\begin{align}s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\end{align}\n\nAnd so s contains enough information to determine that the derivative\nwe want is 1!\n\nOf course this glosses over the challenge of how to actually compute\nthat derivative. The point here is that s is carrying along enough\ninformation that it is possible to compute it. In reality, the\ndevelopers of Pytorch program the sum() and + operations to know how to\ncompute their gradients, and run the back propagation algorithm. An\nin-depth discussion of that algorithm is beyond the scope of this\ntutorial.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Lets have Pytorch compute the gradient, and see that we were right:\n(note if you run this block multiple times, the gradient will increment.\nThat is because Pytorch *accumulates* the gradient into the .grad\nproperty, since for many models this is very convenient.)\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# calling .backward() on any variable will run backprop, starting from it.\ns.backward()\nprint(x.grad)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Understanding what is going on in the block below is crucial for being a\nsuccessful programmer in deep learning.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = torch.randn((2, 2))\ny = torch.randn((2, 2))\nz = x + y  # These are Tensor types, and backprop would not be possible\n\nvar_x = autograd.Variable(x)\nvar_y = autograd.Variable(y)\n# var_z contains enough information to compute gradients, as we saw above\nvar_z = var_x + var_y\nprint(var_z.creator)\n\nvar_z_data = var_z.data  # Get the wrapped Tensor object out of var_z...\n# Re-wrap the tensor in a new variable\nnew_var_z = autograd.Variable(var_z_data)\n\n# ... does new_var_z have information to backprop to x and y?\n# NO!\nprint(new_var_z.creator)\n# And how could it?  We yanked the tensor out of var_z (that is \n# what var_z.data is).  This tensor doesn't know anything about\n# how it was computed.  We pass it into new_var_z, and this is all the\n# information new_var_z gets.  If var_z_data doesn't know how it was \n# computed, theres no way new_var_z will.\n# In essence, we have broken the variable away from its past history"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Here is the basic, extremely important rule for computing with\nautograd.Variables (note this is more general than Pytorch. There is an\nequivalent object in every major deep learning toolkit):\n\n**If you want the error from your loss function to backpropogate to a\ncomponent of your network, you MUST NOT break the Variable chain from\nthat component to your loss Variable. If you do, the loss will have no\nidea your component exists, and its parameters can't be updated.**\n\nI say this in bold, because this error can creep up on you in very\nsubtle ways (I will show some such ways below), and it will not cause\nyour code to crash or complain, so you must be careful.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "3. Deep Learning Building Blocks: Affine maps, non-linearities and objectives\n=============================================================================\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Deep learning consists of composing linearities with non-linearities in\nclever ways. The introduction of non-linearities allows for powerful\nmodels. In this section, we will play with these core components, make\nup an objective function, and see how the model is trained.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Affine Maps\n~~~~~~~~~~~\n\nOne of the core workhorses of deep learning is the affine map, which is\na function $f(x)$ where\n\n\\begin{align}f(x) = Ax + b\\end{align}\n\nfor a matrix $A$ and vectors $x, b$. The parameters to be\nlearned here are $A$ and $b$. Often, $b$ is refered to\nas the *bias* term.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Pytorch and most other deep learning frameworks do things a little\ndifferently than traditional linear algebra. It maps the rows of the\ninput instead of the columns. That is, the $i$'th row of the\noutput below is the mapping of the $i$'th row of the input under\n$A$, plus the bias term. Look at the example below.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\ndata = autograd.Variable(torch.randn(2, 5))\nprint(lin(data))  # yes"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Non-Linearities\n~~~~~~~~~~~~~~~\n\nFirst, note the following fact, which will explain why we need\nnon-linearities in the first place. Suppose we have two affine maps\n$f(x) = Ax + b$ and $g(x) = Cx + d$. What is\n$f(g(x))$?\n\n\\begin{align}f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\end{align}\n\n$AC$ is a matrix and $Ad + b$ is a vector, so we see that\ncomposing affine maps gives you an affine map.\n\nFrom this, you can see that if you wanted your neural network to be long\nchains of affine compositions, that this adds no new power to your model\nthan just doing a single affine map.\n\nIf we introduce non-linearities in between the affine layers, this is no\nlonger the case, and we can build much more powerful models.\n\nThere are a few core non-linearities.\n$\\tanh(x), \\sigma(x), \\text{ReLU}(x)$ are the most common. You are\nprobably wondering: \"why these functions? I can think of plenty of other\nnon-linearities.\" The reason for this is that they have gradients that\nare easy to compute, and computing gradients is essential for learning.\nFor example\n\n\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n\nA quick note: although you may have learned some neural networks in your\nintro to AI class where $\\sigma(x)$ was the default non-linearity,\ntypically people shy away from it in practice. This is because the\ngradient *vanishes* very quickly as the absolute value of the argument\ngrows. Small gradients means it is hard to learn. Most people default to\ntanh or ReLU.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n# Note that non-linearites typically don't have parameters like affine maps do.\n# That is, they don't have weights that are updated during training.\ndata = autograd.Variable(torch.randn(2, 2))\nprint(data)\nprint(F.relu(data))"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Softmax and Probabilities\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe function $\\text{Softmax}(x)$ is also just a non-linearity, but\nit is special in that it usually is the last operation done in a\nnetwork. This is because it takes in a vector of real numbers and\nreturns a probability distribution. Its definition is as follows. Let\n$x$ be a vector of real numbers (positive, negative, whatever,\nthere are no constraints). Then the i'th component of\n$\\text{Softmax}(x)$ is\n\n\\begin{align}\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{align}\n\nIt should be clear that the output is a probability distribution: each\nelement is non-negative and the sum over all components is 1.\n\nYou could also think of it as just applying an element-wise\nexponentiation operator to the input to make everything non-negative and\nthen dividing by the normalization constant.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Softmax is also in torch.functional\ndata = autograd.Variable(torch.randn(5))\nprint(data)\nprint(F.softmax(data))\nprint(F.softmax(data).sum())  # Sums to 1 because it is a distribution!\nprint(F.log_softmax(data))  # theres also log_softmax"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Objective Functions\n~~~~~~~~~~~~~~~~~~~\n\nThe objective function is the function that your network is being\ntrained to minimize (in which case it is often called a *loss function*\nor *cost function*). This proceeds by first choosing a training\ninstance, running it through your neural network, and then computing the\nloss of the output. The parameters of the model are then updated by\ntaking the derivative of the loss function. Intuitively, if your model\nis completely confident in its answer, and its answer is wrong, your\nloss will be high. If it is very confident in its answer, and its answer\nis correct, the loss will be low.\n\nThe idea behind minimizing the loss function on your training examples\nis that your network will hopefully generalize well and have small loss\non unseen examples in your dev set, test set, or in production. An\nexample loss function is the *negative log likelihood loss*, which is a\nvery common objective for multi-class classification. For supervised\nmulti-class classification, this means training the network to minimize\nthe negative log probability of the correct output (or equivalently,\nmaximize the log probability of the correct output).\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "4. Optimization and Training\n============================\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "So what we can compute a loss function for an instance? What do we do\nwith that? We saw earlier that autograd.Variable's know how to compute\ngradients with respect to the things that were used to compute it. Well,\nsince our loss is an autograd.Variable, we can compute gradients with\nrespect to all of the parameters used to compute it! Then we can perform\nstandard gradient updates. Let $\\theta$ be our parameters,\n$L(\\theta)$ the loss function, and $\\eta$ a positive\nlearning rate. Then:\n\n\\begin{align}\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\end{align}\n\nThere are a huge collection of algorithms and active research in\nattempting to do something more than just this vanilla gradient update.\nMany attempt to vary the learning rate based on what is happening at\ntrain time. You don't need to worry about what specifically these\nalgorithms are doing unless you are really interested. Torch provies\nmany in the torch.optim package, and they are all completely\ntransparent. Using the simplest gradient update is the same as the more\ncomplicated algorithms. Trying different update algorithms and different\nparameters for the update algorithms (like different initial learning\nrates) is important in optimizing your network's performance. Often,\njust replacing vanilla SGD with an optimizer like Adam or RMSProp will\nboost performance noticably.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "5. Creating Network Components in Pytorch\n=========================================\n\nBefore we move on to our focus on NLP, lets do an annotated example of\nbuilding a network in Pytorch using only affine maps and\nnon-linearities. We will also see how to compute a loss function, using\nPytorch's built in negative log likelihood, and update parameters by\nbackpropagation.\n\nAll network components should inherit from nn.Module and override the\nforward() method. That is about it, as far as the boilerplate is\nconcerned. Inheriting from nn.Module provides functionality to your\ncomponent. For example, it makes it keep track of its trainable\nparameters, you can swap it between CPU and GPU with the .cuda() or\n.cpu() functions, etc.\n\nLet's write an annotated example of a network that takes in a sparse\nbag-of-words representation and outputs a probability distribution over\ntwo labels: \"English\" and \"Spanish\". This model is just logistic\nregression.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Example: Logistic Regression Bag-of-Words classifier\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOur model will map a sparse BOW representation to log probabilities over\nlabels. We assign each word in the vocab an index. For example, say our\nentire vocab is two words \"hello\" and \"world\", with indices 0 and 1\nrespectively. The BoW vector for the sentence \"hello hello hello hello\"\nis\n\n\\begin{align}\\left[ 4, 0 \\right]\\end{align}\n\nFor \"hello world world hello\", it is\n\n\\begin{align}\\left[ 2, 2 \\right]\\end{align}\n\netc. In general, it is\n\n\\begin{align}\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\end{align}\n\nDenote this BOW vector as $x$. The output of our network is:\n\n\\begin{align}\\log \\text{Softmax}(Ax + b)\\end{align}\n\nThat is, we pass the input through an affine map and then do log\nsoftmax.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n        (\"Give it to me\".split(), \"ENGLISH\"),\n        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n\ntest_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n             (\"it is lost on me\".split(), \"ENGLISH\")]\n\n# word_to_ix maps each word in the vocab to a unique integer, which will be its\n# index into the Bag of words vector\nword_to_ix = {}\nfor sent, _ in data + test_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\nprint(word_to_ix)\n\nVOCAB_SIZE = len(word_to_ix)\nNUM_LABELS = 2\n\n\nclass BoWClassifier(nn.Module):  # inheriting from nn.Module!\n\n    def __init__(self, num_labels, vocab_size):\n        # calls the init function of nn.Module.  Dont get confused by syntax,\n        # just always do it in an nn.Module\n        super(BoWClassifier, self).__init__()\n\n        # Define the parameters that you will need.  In this case, we need A and b,\n        # the parameters of the affine mapping.\n        # Torch defines nn.Linear(), which provides the affine map.\n        # Make sure you understand why the input dimension is vocab_size\n        # and the output is num_labels!\n        self.linear = nn.Linear(vocab_size, num_labels)\n\n        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n        # to worry about that here\n\n    def forward(self, bow_vec):\n        # Pass the input through the linear layer,\n        # then pass that through log_softmax.\n        # Many non-linearities and other functions are in torch.nn.functional\n        return F.log_softmax(self.linear(bow_vec))\n\n\ndef make_bow_vector(sentence, word_to_ix):\n    vec = torch.zeros(len(word_to_ix))\n    for word in sentence:\n        vec[word_to_ix[word]] += 1\n    return vec.view(1, -1)\n\n\ndef make_target(label, label_to_ix):\n    return torch.LongTensor([label_to_ix[label]])\n\nmodel = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n\n# the model knows its parameters.  The first output below is A, the second is b.\n# Whenever you assign a component to a class variable in the __init__ function\n# of a module, which was done with the line\n# self.linear = nn.Linear(...)\n# Then through some Python magic from the Pytorch devs, your module\n#(in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\nfor param in model.parameters():\n    print(param)\n\n# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\nsample = data[0]\nbow_vector = make_bow_vector(sample[0], word_to_ix)\nlog_probs = model(autograd.Variable(bow_vector))\nprint(log_probs)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Which of the above values corresponds to the log probability of ENGLISH,\nand which to SPANISH? We never defined it, but we need to if we want to\ntrain the thing.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
      ]
    },
    {
      "metadata": {},
      "source": [
        "So lets train! To do this, we pass instances through to get log\nprobabilities, compute a loss function, compute the gradient of the loss\nfunction, and then update the parameters with a gradient step. Loss\nfunctions are provided by Torch in the nn package. nn.NLLLoss() is the\nnegative log likelihood loss we want. It also defines optimization\nfunctions in torch.optim. Here, we will just use SGD.\n\nNote that the *input* to NLLLoss is a vector of log probabilities, and a\ntarget label. It doesn't compute the log probabilities for us. This is\nwhy the last layer of our network is log softmax. The loss function\nnn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log\nsoftmax for you.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Run on test data before we train, just to see a before-and-after\nfor instance, label in test_data:\n    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n    log_probs = model(bow_vec)\n    print(log_probs)\n\n# Print the matrix column corresponding to \"creo\"\nprint(next(model.parameters())[:, word_to_ix[\"creo\"]])\n\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# Usually you want to pass over the training data several times.\n# 100 is much bigger than on a real data set, but real datasets have more than\n# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\nfor epoch in range(100):\n    for instance, label in data:\n        # Step 1. Remember that Pytorch accumulates gradients.\n        # We need to clear them out before each instance\n        model.zero_grad()\n\n        # Step 2. Make our BOW vector and also we must wrap the target in a\n        # Variable as an integer. For example, if the target is SPANISH, then\n        # we wrap the integer 0. The loss function then knows that the 0th\n        # element of the log probabilities is the log probability\n        # corresponding to SPANISH\n        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n        target = autograd.Variable(make_target(label, label_to_ix))\n\n        # Step 3. Run our forward pass.\n        log_probs = model(bow_vec)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by \n        # calling optimizer.step()\n        loss = loss_function(log_probs, target)\n        loss.backward()\n        optimizer.step()\n\nfor instance, label in test_data:\n    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n    log_probs = model(bow_vec)\n    print(log_probs)\n\n# Index corresponding to Spanish goes up, English goes down!\nprint(next(model.parameters())[:, word_to_ix[\"creo\"]])"
      ]
    },
    {
      "metadata": {},
      "source": [
        "We got the right answer! You can see that the log probability for\nSpanish is much higher in the first example, and the log probability for\nEnglish is much higher in the second for the test data, as it should be.\n\nNow you see how to make a Pytorch component, pass some data through it\nand do gradient updates. We are ready to dig deeper into what deep NLP\nhas to offer.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "6. Word Embeddings: Encoding Lexical Semantics\n==============================================\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Word embeddings are dense vectors of real numbers, one per word in your\nvocabulary. In NLP, it is almost always the case that your features are\nwords! But how should you represent a word in a computer? You could\nstore its ascii character representation, but that only tells you what\nthe word *is*, it doesn't say much about what it *means* (you might be\nable to derive its part of speech from its affixes, or properties from\nits capitalization, but not much). Even more, in what sense could you\ncombine these representations? We often want dense outputs from our\nneural networks, where the inputs are $|V|$ dimensional, where\n$V$ is our vocabulary, but often the outputs are only a few\ndimensional (if we are only predicting a handful of labels, for\ninstance). How do we get from a massive dimensional space to a smaller\ndimensional space?\n\nHow about instead of ascii representations, we use a one-hot encoding?\nThat is, we represent the word $w$ by\n\n\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n\nwhere the 1 is in a location unique to $w$. Any other word will\nhave a 1 in some other location, and a 0 everywhere else.\n\nThere is an enormous drawback to this representation, besides just how\nhuge it is. It basically treats all words as independent entities with\nno relation to each other. What we really want is some notion of\n*similarity* between words. Why? Let's see an example.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Suppose we are building a language model. Suppose we have seen the\nsentences \n\n* The mathematician ran to the store. \n* The physicist ran to the store. \n* The mathematician solved the open problem.\n\nin our training data. Now suppose we get a new sentence never before\nseen in our training data: \n\n* The physicist solved the open problem.\n\nOur language model might do OK on this sentence, but wouldn't it be much\nbetter if we could use the following two facts: \n\n* We have seen  mathematician and physicist in the same role in a sentence. Somehow they\n  have a semantic relation. \n* We have seen mathematician in the same role  in this new unseen sentence \n  as we are now seeing physicist.\n\nand then infer that physicist is actually a good fit in the new unseen\nsentence? This is what we mean by a notion of similarity: we mean\n*semantic similarity*, not simply having similar orthographic\nrepresentations. It is a technique to combat the sparsity of linguistic\ndata, by connecting the dots between what we have seen and what we\nhaven't. This example of course relies on a fundamental linguistic\nassumption: that words appearing in similar contexts are related to each\nother semantically. This is called the `distributional\nhypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Getting Dense Word Embeddings\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nHow can we solve this problem? That is, how could we actually encode\nsemantic similarity in words? Maybe we think up some semantic\nattributes. For example, we see that both mathematicians and physicists\ncan run, so maybe we give these words a high score for the \"is able to\nrun\" semantic attribute. Think of some other attributes, and imagine\nwhat you might score some common words on those attributes.\n\nIf each attribute is a dimension, then we might give each word a vector,\nlike this:\n\n\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n\n\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n\nThen we can get a measure of similarity between these words by doing:\n\n\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}\n\nAlthough it is more common to normalize by the lengths:\n\n\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n   {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}\n\nWhere $\\phi$ is the angle between the two vectors. That way,\nextremely similar words (words whose embeddings point in the same\ndirection) will have similarity 1. Extremely dissimilar words should\nhave similarity -1.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "You can think of the sparse one-hot vectors from the beginning of this\nsection as a special case of these new vectors we have defined, where\neach word basically has similarity 0, and we gave each word some unique\nsemantic attribute. These new vectors are *dense*, which is to say their\nentries are (typically) non-zero.\n\nBut these new vectors are a big pain: you could think of thousands of\ndifferent semantic attributes that might be relevant to determining\nsimilarity, and how on earth would you set the values of the different\nattributes? Central to the idea of deep learning is that the neural\nnetwork learns representations of the features, rather than requiring\nthe programmer to design them herself. So why not just let the word\nembeddings be parameters in our model, and then be updated during\ntraining? This is exactly what we will do. We will have some *latent\nsemantic attributes* that the network can, in principle, learn. Note\nthat the word embeddings will probably not be interpretable. That is,\nalthough with our hand-crafted vectors above we can see that\nmathematicians and physicists are similar in that they both like coffee,\nif we allow a neural network to learn the embeddings and see that both\nmathematicians and physicisits have a large value in the second\ndimension, it is not clear what that means. They are similar in some\nlatent semantic dimension, but this probably has no interpretation to\nus.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "In summary, **word embeddings are a representation of the *semantics* of\na word, efficiently encoding semantic information that might be relevant\nto the task at hand**. You can embed other things too: part of speech\ntags, parse trees, anything! The idea of feature embeddings is central\nto the field.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Word Embeddings in Pytorch\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore we get to a worked example and an exercise, a few quick notes\nabout how to use embeddings in Pytorch and in deep learning programming\nin general. Similar to how we defined a unique index for each word when\nmaking one-hot vectors, we also need to define an index for each word\nwhen using embeddings. These will be keys into a lookup table. That is,\nembeddings are stored as a $|V| \\times D$ matrix, where $D$\nis the dimensionality of the embeddings, such that the word assigned\nindex $i$ has its embedding stored in the $i$'th row of the\nmatrix. In all of my code, the mapping from words to indices is a\ndictionary named word\\_to\\_ix.\n\nThe module that allows you to use embeddings is torch.nn.Embedding,\nwhich takes two arguments: the vocabulary size, and the dimensionality\nof the embeddings.\n\nTo index into this table, you must use torch.LongTensor (since the\nindices are integers, not floats).\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\nembeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\nlookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\nhello_embed = embeds(autograd.Variable(lookup_tensor))\nprint(hello_embed)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "An Example: N-Gram Language Modeling\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRecall that in an n-gram language model, given a sequence of words\n$w$, we want to compute\n\n\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n\nWhere $w_i$ is the ith word of the sequence.\n\nIn this example, we will compute the loss function on some training\nexamples and update the parameters with backpropagation.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\nEMBEDDING_DIM = 10\n# We will use Shakespeare Sonnet 2\ntest_sentence = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\".split()\n# we should tokenize the input, but we will ignore that for now\n# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\ntrigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\n# print the first 3, just so you can see what they look like\nprint(trigrams[:3])\n\nvocab = set(test_sentence)\nword_to_ix = {word: i for i, word in enumerate(vocab)}\n\n\nclass NGramLanguageModeler(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, context_size):\n        super(NGramLanguageModeler, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n        self.linear2 = nn.Linear(128, vocab_size)\n\n    def forward(self, inputs):\n        embeds = self.embeddings(inputs).view((1, -1))\n        out = F.relu(self.linear1(embeds))\n        out = self.linear2(out)\n        log_probs = F.log_softmax(out)\n        return log_probs\n\nlosses = []\nloss_function = nn.NLLLoss()\nmodel = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(10):\n    total_loss = torch.Tensor([0])\n    for context, target in trigrams:\n\n        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n        # into integer indices and wrap them in variables)\n        context_idxs = [word_to_ix[w] for w in context]\n        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n\n        # Step 2. Recall that torch *accumulates* gradients.  Before passing in a new instance,\n        # you need to zero out the gradients from the old instance\n        model.zero_grad()\n\n        # Step 3. Run the forward pass, getting log probabilities over next\n        # words\n        log_probs = model(context_var)\n\n        # Step 4. Compute your loss function. (Again, Torch wants the target\n        # word wrapped in a variable)\n        loss = loss_function(log_probs, autograd.Variable(\n            torch.LongTensor([word_to_ix[target]])))\n\n        # Step 5. Do the backward pass and update the gradient\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.data\n    losses.append(total_loss)\nprint(losses)  # The loss decreased every iteration over the training data!"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\nlearning. It is a model that tries to predict words given the context of\na few words before and a few words after the target word. This is\ndistinct from language modeling, since CBOW is not sequential and does\nnot have to be probabilistic. Typcially, CBOW is used to quickly train\nword embeddings, and these embeddings are used to initialize the\nembeddings of some more complicated model. Usually, this is referred to\nas *pretraining embeddings*. It almost always helps performance a couple\nof percent.\n\nThe CBOW model is as follows. Given a target word $w_i$ and an\n$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\nand $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\ncollectively as $C$, CBOW tries to minimize\n\n\\begin{align}-\\log p(w_i | C) = \\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n\nwhere $q_w$ is the embedding for word $w$.\n\nImplement this model in Pytorch by filling in the class below. Some\ntips: \n\n* Think about which parameters you need to define. \n* Make sure you know what shape each operation expects. Use .view() if you need to\n  reshape.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\nraw_text = \"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\".split()\nword_to_ix = {word: i for i, word in enumerate(raw_text)}\ndata = []\nfor i in range(2, len(raw_text) - 2):\n    context = [raw_text[i - 2], raw_text[i - 1],\n               raw_text[i + 1], raw_text[i + 2]]\n    target = raw_text[i]\n    data.append((context, target))\nprint(data[:5])\n\n\nclass CBOW(nn.Module):\n\n    def __init__(self):\n        pass\n\n    def forward(self, inputs):\n        pass\n\n# create your model and train.  here are some functions to help you make\n# the data ready for use by your module\n\n\ndef make_context_vector(context, word_to_ix):\n    idxs = [word_to_ix[w] for w in context]\n    tensor = torch.LongTensor(idxs)\n    return autograd.Variable(tensor)\n\nmake_context_vector(data[0][0], word_to_ix)  # example"
      ]
    },
    {
      "metadata": {},
      "source": [
        "7. Sequence Models and Long-Short Term Memory Networks\n======================================================\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "At this point, we have seen various feed-forward networks. That is,\nthere is no state maintained by the network at all. This might not be\nthe behavior we want. Sequence models are central to NLP: they are\nmodels where there is some sort of dependence through time between your\ninputs. The classical example of a sequence model is the Hidden Markov\nModel for part-of-speech tagging. Another example is the conditional\nrandom field.\n\nA recurrent neural network is a network that maintains some kind of\nstate. For example, its output could be used as part of the next input,\nso that information can propogate along as the network passes over the\nsequence. In the case of an LSTM, for each element in the sequence,\nthere is a corresponding *hidden state* $h_t$, which in principle\ncan contain information from arbitrary points earlier in the sequence.\nWe can use the hidden state to predict words in a language model,\npart-of-speech tags, and a myriad of other things.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "LSTM's in Pytorch\n~~~~~~~~~~~~~~~~~\n\nBefore getting to the example, note a few things. Pytorch's LSTM expects\nall of its inputs to be 3D tensors. The semantics of the axes of these\ntensors is important. The first axis is the sequence itself, the second\nindexes instances in the mini-batch, and the third indexes elements of\nthe input. We haven't discussed mini-batching, so lets just ignore that\nand assume we will always have just 1 dimension on the second axis. If\nwe want to run the sequence model over the sentence \"The cow jumped\",\nour input should look like\n\n\\begin{align}\\begin{bmatrix}\n   \\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n   q_\\text{cow} \\\\\n   q_\\text{jumped}\n   \\end{bmatrix}\\end{align}\n\nExcept remember there is an additional 2nd dimension with size 1.\n\nIn addition, you could go through the sequence one at a time, in which\ncase the 1st axis will have size 1 also.\n\nLet's see a quick example.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\ninputs = [autograd.Variable(torch.randn((1, 3)))\n          for _ in range(5)]  # make a sequence of length 5\n\n# initialize the hidden state.\nhidden = (autograd.Variable(torch.randn(1, 1, 3)),\n          autograd.Variable(torch.randn((1, 1, 3))))\nfor i in inputs:\n    # Step through the sequence one element at a time.\n    # after each step, hidden contains the hidden state.\n    out, hidden = lstm(i.view(1, 1, -1), hidden)\n\n# alternatively, we can do the entire sequence all at once.\n# the first value returned by LSTM is all of the hidden states throughout \n# the sequence. the second is just the most recent hidden state \n# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n# The reason for this is that:\n# \"out\" will give you access to all hidden states in the sequence\n# \"hidden\" will allow you to continue the sequence and backpropogate, \n# by passing it as an argument  to the lstm at a later time\n# Add the extra 2nd dimension\ninputs = torch.cat(inputs).view(len(inputs), 1, -1)\nhidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(\n    torch.randn((1, 1, 3))))  # clean out hidden state\nout, hidden = lstm(inputs, hidden)\nprint(out)\nprint(hidden)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Example: An LSTM for Part-of-Speech Tagging\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "In this section, we will use an LSTM to get part of speech tags. We will\nnot use Viterbi or Forward-Backward or anything like that, but as a\n(challenging) exercise to the reader, think about how Viterbi could be\nused after you have seen what is going on.\n\nThe model is as follows: let our input sentence be\n$w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. Also, let\n$T$ be our tag set, and $y_i$ the tag of word $w_i$.\nDenote our prediction of the tag of word $w_i$ by\n$\\hat{y}_i$.\n\nThis is a structure prediction, model, where our output is a sequence\n$\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$.\n\nTo do the prediction, pass an LSTM over the sentence. Denote the hidden\nstate at timestep $i$ as $h_i$. Also, assign each tag a\nunique index (like how we had word\\_to\\_ix in the word embeddings\nsection). Then our prediction rule for $\\hat{y}_i$ is\n\n\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\\end{align}\n\nThat is, take the log softmax of the affine map of the hidden state,\nand the predicted tag is the tag that has the maximum value in this\nvector. Note this implies immediately that the dimensionality of the\ntarget space of $A$ is $|T|$.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    tensor = torch.LongTensor(idxs)\n    return autograd.Variable(tensor)\n\ntraining_data = [\n    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n]\nword_to_ix = {}\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\nprint(word_to_ix)\ntag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n\n# These will usually be more like 32 or 64 dimensional.\n# We will keep them small, so we can see how the weights change as we train.\nEMBEDDING_DIM = 6\nHIDDEN_DIM = 6\n\n\nclass LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # Before we've done anything, we dont have any hidden state.\n        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1))\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_space)\n        return tag_scores\n\nmodel = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# See what the scores are before training\n# Note that element i,j of the output is the score for tag j for word i.\ninputs = prepare_sequence(training_data[0][0], word_to_ix)\ntag_scores = model(inputs)\nprint(tag_scores)\n\nfor epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n    for sentence, tags in training_data:\n        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n        # before each instance\n        model.zero_grad()\n\n        # Also, we need to clear out the hidden state of the LSTM, detaching it from its\n        # history on the last instance.\n        model.hidden = model.init_hidden()\n\n        # Step 2. Get our inputs ready for the network, that is, turn them into Variables\n        # of word indices.\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n\n        # Step 3. Run our forward pass.\n        tag_scores = model(sentence_in)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by calling\n        # optimizer.step()\n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n\n# See what the scores are after training\ninputs = prepare_sequence(training_data[0][0], word_to_ix)\ntag_scores = model(inputs)\n# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j for word i.\n# The predicted tag is the maximum scoring tag.\n# Here, we can see the predicted sequence below is 0 1 2 0 1\n# since 0 is index of the maximum value of row 1,\n# 1 is the index of maximum value of row 2, etc.\n# Which is DET NOUN VERB DET NOUN, the correct sequence!\nprint(tag_scores)"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Exercise: Augmenting the LSTM part-of-speech tagger with character-level features\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn the example above, each word had an embedding, which served as the\ninputs to our sequence model. Let's augment the word embeddings with a\nrepresentation derived from the characters of the word. We expect that\nthis should help significantly, since character-level information like\naffixes have a large bearing on part-of-speech. For example, words with\nthe affix *-ly* are almost always tagged as adverbs in English.\n\nDo do this, let $c_w$ be the character-level representation of\nword $w$. Let $x_w$ be the word embedding as before. Then\nthe input to our sequence model is the concatenation of $x_w$ and\n$c_w$. So if $x_w$ has dimension 5, and $c_w$\ndimension 3, then our LSTM should accept an input of dimension 8.\n\nTo get the character level representation, do an LSTM over the\ncharacters of a word, and let $c_w$ be the final hidden state of\nthis LSTM. Hints: \n* There are going to be two LSTM's in your new model.\n  The original one that outputs POS tag scores, and the new one that\n  outputs a character-level representation of each word.\n* To do a sequence model over characters, you will have to embed characters.\n  The character embeddings will be the input to the character LSTM.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "8. Advanced: Making Dynamic Decisions and the Bi-LSTM CRF\n=========================================================\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Dyanmic versus Static Deep Learning Toolkits\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPytorch is a *dynamic* neural network kit. Another example of a dynamic\nkit is `Dynet <https://github.com/clab/dynet>`__ (I mention this because\nworking with Pytorch and Dynet is similar. If you see an example in\nDynet, it will probably help you implement it in Pytorch). The opposite\nis the *static* tool kit, which includes Theano, Keras, TensorFlow, etc.\nThe core difference is the following: \n\n* In a static toolkit, you define\n  a computation graph once, compile it, and then stream instances to it.\n* In a dynamic toolkit, you define a computation graph *for each\n  instance*. It is never compiled and is executed on-the-fly\n\nWithout a lot of experience, it is difficult to appreciate the\ndifference. One example is to suppose we want to build a deep\nconstituent parser. Suppose our model involves roughly the following\nsteps: \n\n* We build the tree bottom up \n* Tag the root nodes (the words of the sentence) \n* From there, use a neural network and the embeddings\n\nof the words to find combinations that form constituents. Whenever you\nform a new constituent, use some sort of technique to get an embedding\nof the constituent. In this case, our network architecture will depend\ncompletely on the input sentence. In the sentence \"The green cat\nscratched the wall\", at some point in the model, we will want to combine\nthe span $(i,j,r) = (1, 3, \\text{NP})$ (that is, an NP constituent\nspans word 1 to word 3, in this case \"The green cat\").\n\nHowever, another sentence might be \"Somewhere, the big fat cat scratched\nthe wall\". In this sentence, we will want to form the constituent\n$(2, 4, NP)$ at some point. The constituents we will want to form\nwill depend on the instance. If we just compile the computation graph\nonce, as in a static toolkit, it will be exceptionally difficult or\nimpossible to program this logic. In a dynamic toolkit though, there\nisn't just 1 pre-defined computation graph. There can be a new\ncomputation graph for each instance, so this problem goes away.\n\nDynamic toolkits also have the advantage of being easier to debug and\nthe code more closely resembling the host language (by that I mean that\nPytorch and Dynet look more like actual Python code than Keras or\nTheano).\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Bi-LSTM Conditional Random Field Discussion\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor this section, we will see a full, complicated example of a Bi-LSTM\nConditional Random Field for named-entity recognition. The LSTM tagger\nabove is typically sufficient for part-of-speech tagging, but a sequence\nmodel like the CRF is really essential for strong performance on NER.\nFamiliarity with CRF's is assumed. Although this name sounds scary, all\nthe model is is a CRF but where an LSTM provides the features. This is\nan advanced model though, far more complicated than any earlier model in\nthis tutorial. If you want to skip it, that is fine. To see if you're\nready, see if you can:\n\n-  Write the recurrence for the viterbi variable at step i for tag k.\n-  Modify the above recurrence to compute the forward variables instead.\n-  Modify again the above recurrence to compute the forward variables in\n   log-space (hint: log-sum-exp)\n\nIf you can do those three things, you should be able to understand the\ncode below. Recall that the CRF computes a conditional probability. Let\n$y$ be a tag sequence and $x$ an input sequence of words.\nThen we compute\n\n\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}\n\nWhere the score is determined by defining some log potentials\n$\\log \\psi_i(x,y)$ such that\n\n\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\end{align}\n\nTo make the partition function tractable, the potentials must look only\nat local features.\n\nIn the Bi-LSTM CRF, we define two kinds of potentials: emission and\ntransition. The emission potential for the word at index $i$ comes\nfrom the hidden state of the Bi-LSTM at timestep $i$. The\ntransition scores are stored in a $|T|x|T|$ matrix\n$\\textbf{P}$, where $T$ is the tag set. In my\nimplementation, $\\textbf{P}_{j,k}$ is the score of transitioning\nto tag $j$ from tag $k$. So:\n\n\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}\n\n\\begin{align}= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\end{align}\n\nwhere in this second expression, we think of the tags as being assigned\nunique non-negative indices.\n\nIf the above discussion was too brief, you can check out\n`this <http://www.cs.columbia.edu/%7Emcollins/crf.pdf>`__ write up from\nMichael Collins on CRFs.\n\nImplementation Notes\n~~~~~~~~~~~~~~~~~~~~\n\nThe example below implements the forward algorithm in log space to\ncompute the partition function, and the viterbi algorithm to decode.\nBackpropagation will compute the gradients automatically for us. We\ndon't have to do anything by hand.\n\nThe implementation is not optimized. If you understand what is going on,\nyou'll probably quickly see that iterating over the next tag in the\nforward algorithm could probably be done in one big operation. I wanted\nto code to be more readable. If you want to make the relevant change,\nyou could probably use this tagger for real tasks.\n\n\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Helper functions to make the code more readable.\ndef to_scalar(var):\n    # returns a python float\n    return var.view(-1).data.tolist()[0]\n\n\ndef argmax(vec):\n    # return the argmax as a python int\n    _, idx = torch.max(vec, 1)\n    return to_scalar(idx)\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\n\n\ndef log_sum_exp(vec):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n\n\nclass BiLSTM_CRF(nn.Module):\n\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM_CRF, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim / 2,\n                            num_layers=1, bidirectional=True)\n\n        # Maps the output of the LSTM into tag space.\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n\n        # Matrix of transition parameters.  Entry i,j is the score of\n        # transitioning *to* i *from* j.\n        self.transitions = nn.Parameter(\n            torch.randn(self.tagset_size, self.tagset_size))\n\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        return (autograd.Variable(torch.randn(2, 1, self.hidden_dim)),\n                autograd.Variable(torch.randn(2, 1, self.hidden_dim)))\n\n    def _forward_alg(self, feats):\n        # Do the forward algorithm to compute the partition function\n        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n        # START_TAG has all of the score.\n        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n\n        # Wrap in a variable so that we will get automatic backprop\n        forward_var = autograd.Variable(init_alphas)\n\n        # Iterate through the sentence\n        for feat in feats:\n            alphas_t = []  # The forward variables at this timestep\n            for next_tag in range(self.tagset_size):\n                # broadcast the emission score: it is the same regardless of\n                # the previous tag\n                emit_score = feat[next_tag].view(\n                    1, -1).expand(1, self.tagset_size)\n                # the ith entry of trans_score is the score of transitioning to\n                # next_tag from i\n                trans_score = self.transitions[next_tag].view(1, -1)\n                # The ith entry of next_tag_var is the value for the edge (i -> next_tag)\n                # before we do log-sum-exp\n                next_tag_var = forward_var + trans_score + emit_score\n                # The forward variable for this tag is log-sum-exp of all the\n                # scores.\n                alphas_t.append(log_sum_exp(next_tag_var))\n            forward_var = torch.cat(alphas_t).view(1, -1)\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        alpha = log_sum_exp(terminal_var)\n        return alpha\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n\n    def _score_sentence(self, feats, tags):\n        # Gives the score of a provided tag sequence\n        score = autograd.Variable(torch.Tensor([0]))\n        tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n        for i, feat in enumerate(feats):\n            score = score + \\\n                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n        return score\n\n    def _viterbi_decode(self, feats):\n        backpointers = []\n\n        # Initialize the viterbi variables in log space\n        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n\n        # forward_var at step i holds the viterbi variables for step i-1\n        forward_var = autograd.Variable(init_vvars)\n        for feat in feats:\n            bptrs_t = []  # holds the backpointers for this step\n            viterbivars_t = []  # holds the viterbi variables for this step\n\n            for next_tag in range(self.tagset_size):\n                # next_tag_var[i] holds the viterbi variable for tag i at the previous step,\n                # plus the score of transitioning from tag i to next_tag.\n                # We don't include the emission scores here because the max\n                # does not depend on them (we add them in below)\n                next_tag_var = forward_var + self.transitions[next_tag]\n                best_tag_id = argmax(next_tag_var)\n                bptrs_t.append(best_tag_id)\n                viterbivars_t.append(next_tag_var[0][best_tag_id])\n            # Now add in the emission scores, and assign forward_var to the set\n            # of viterbi variables we just computed\n            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append(bptrs_t)\n\n        # Transition to STOP_TAG\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        best_tag_id = argmax(terminal_var)\n        path_score = terminal_var[0][best_tag_id]\n\n        # Follow the back pointers to decode the best path.\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n        # Pop off the start tag (we dont want to return that to the caller)\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n        best_path.reverse()\n        return path_score, best_path\n\n    def neg_log_likelihood(self, sentence, tags):\n        self.hidden = self.init_hidden()\n        feats = self._get_lstm_features(sentence)\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n\n    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n        self.hidden = self.init_hidden()\n        # Get the emission scores from the BiLSTM\n        lstm_feats = self._get_lstm_features(sentence)\n\n        # Find the best path, given the features.\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq\n\n\nSTART_TAG = \"<START>\"\nSTOP_TAG = \"<STOP>\"\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4\n\n# Make up some training data\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n), (\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]\n\nword_to_ix = {}\nfor sentence, tags in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n\nmodel = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n\n# Check predictions before training\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\nprecheck_tags = torch.LongTensor([tag_to_ix[t] for t in training_data[0][1]])\nprint(model(precheck_sent))\n\n# Make sure prepare_sequence from earlier in the LSTM section is loaded\nfor epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n    for sentence, tags in training_data:\n        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n        # before each instance\n        model.zero_grad()\n\n        # Step 2. Get our inputs ready for the network, that is, turn them into Variables\n        # of word indices.\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = torch.LongTensor([tag_to_ix[t] for t in tags])\n\n        # Step 3. Run our forward pass.\n        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by calling\n        # optimizer.step()\n        neg_log_likelihood.backward()\n        optimizer.step()\n\n# Check predictions after training\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\nprint(model(precheck_sent))\n# We got it!"
      ]
    },
    {
      "metadata": {},
      "source": [
        "Exercise: A new loss function for discriminative tagging\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt wasn't really necessary for us to create a computation graph when\ndoing decoding, since we do not backpropagate from the viterbi path\nscore. Since we have it anyway, try training the tagger where the loss\nfunction is the difference between the Viterbi path score and the score\nof the gold-standard path. It should be clear that this function is\nnon-negative and 0 when the predicted tag sequence is the correct tag\nsequence. This is essentially *structured perceptron*.\n\nThis modification should be short, since Viterbi and score\\_sentence are\nalready implemented. This is an example of the shape of the computation\ngraph *depending on the training instance*. Although I haven't tried\nimplementing this in a static toolkit, I imagine that it is possible but\nmuch less straightforward.\n\nPick up some real data and do a comparison!\n\n\n"
      ],
      "cell_type": "markdown"
    }
  ],
  "nbformat_minor": 0
}